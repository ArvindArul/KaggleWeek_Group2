{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for working and clean functions / loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identified **Next Steps** from this:\n",
    "- For the chunks, use the min function and:\n",
    "    - store indexes with no match at end of loop\n",
    "    - create new chunks with only those indexes\n",
    "    - call again the function to find new pais\n",
    "    - repeat until no indexes left\n",
    "- Find how to creates pairs of Portraits if dealing with mixed files (Ps and Ls)\n",
    "    - Logic is probably like checking the type and if P it has to be followed by a P only if type(i-1) != P\n",
    "- Find how sorting the data can help to go faster or create better pairs\n",
    "- Try to create a ML model based on new table we can create from extracting 10k first pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# below loop will tell you how many cores your have in your CPU. \n",
    "# with mine (11) cores the function runs in 3-4mins\n",
    "# if you have a windows try to check if you can leverage the power of your GPU?\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "num_threads = cpu_count()\n",
    "print(num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file\n",
    "with open(\"../../data/1_binary_landscapes.txt\", \"r\") as file:\n",
    "    num_rows = int(file.readline())\n",
    "    data = []\n",
    "\n",
    "    for _ in range(num_rows):\n",
    "        line = file.readline().strip().split()\n",
    "        picture_type = line[0]\n",
    "        num_tags = int(line[1])\n",
    "        tags = line[2:]\n",
    "        \n",
    "        data.append((picture_type, num_tags, tags))\n",
    "\n",
    "data_with_index = []\n",
    "for index, row in enumerate(data):\n",
    "    new_row = (index,) + row\n",
    "    data_with_index.append(new_row)\n",
    "\n",
    "sorted_data = sorted(data_with_index, key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100\n",
    "\n",
    "# Divide the data into chunks\n",
    "chunks = [sorted_data[i:i+chunk_size] for i in range(0, len(sorted_data), chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 28521, 1: 31633, 2: 18356, 3: 27091, 4: 76, 5: 56707, 6: 7482, 7: 18741, 8: 12396, 9: 6049, 10: 52640, 11: 51930, 12: 29539, 13: 54702, 14: 4348, 15: 7595, 16: 3186, 17: 1789, 18: 40358, 19: 3353, 20: 30442, 21: 9150, 22: 16154, 23: 22684, 24: 17309, 25: 26146, 26: 35510, 27: 52263, 28: 65050, 29: 27533, 30: 18011, 31: 46924, 32: 20946, 33: 53168, 34: 22793, 35: 40423, 36: 12017, 37: 47000, 38: 19569, 39: 699, 40: 29828, 41: 57617, 42: 12500, 43: 16866, 44: 2206, 45: 15492, 46: 40244, 47: 49419, 48: 1160, 49: 24867, 50: 29035, 51: 49513, 52: 55499, 53: 5980, 54: 895, 55: 26742, 56: 8754, 57: 13153, 58: 58725, 59: 19195, 60: 69016, 61: 26072, 62: 16878, 63: 16226, 64: 47736, 65: 46724, 66: 5753, 67: 3294, 68: 8103, 69: 8971, 70: 14017, 71: 24920, 72: 27154, 73: 16871, 74: 12375, 75: 28372, 77: 18997, 78: 1467, 79: 22666, 80: 17792, 81: 27289, 82: 24687, 83: 30729, 84: 7594, 85: 53305, 86: 31315, 87: 18408, 88: 29678, 89: 7592, 90: 54812, 91: 26739, 92: 1124, 93: 39535, 94: 31042, 95: 32033, 96: 41425, 97: 4086, 98: 31516, 99: 45084, 100: 18194, 101: 1455, 102: 31388, 103: 11033, 104: 36068, 105: 13827, 106: 27072, 107: 11184, 108: 37729, 109: 36406, 110: 66145, 111: 24125, 112: 24693, 113: 9630, 114: 6862, 115: 31125, 116: 35015, 117: 24246, 118: 45314, 119: 31820, 120: 8895, 121: 25507, 122: 43312, 123: 8153, 124: 53201, 125: 1840, 126: 39109, 127: 31923, 128: 46645, 129: 7698, 130: 11907, 131: 564, 132: 33131, 133: 40794, 134: 11504, 135: 13797, 136: 13382, 137: 19385, 138: 20407, 139: 38474, 140: 46464, 141: 17157, 142: 54916, 143: 22196, 144: 18818, 145: 48948, 146: 19471, 147: 3361, 148: 59110, 149: 11205, 150: 34772, 151: 375, 152: 2405, 153: 60838, 154: 32852, 155: 932, 156: 33589, 157: 10097, 158: 64311, 159: 58606, 160: 11731, 161: 11269, 162: 7310, 163: 22248, 164: 32212, 165: 49573, 166: 52962, 167: 3001, 168: 28982, 169: 57643, 170: 19104, 171: 49572, 172: 2911, 173: 19072, 174: 6554, 175: 24911, 176: 54854, 177: 43877, 178: 55039, 179: 41732, 180: 35192, 181: 20455, 182: 46525, 183: 12879, 184: 33671, 185: 11661, 186: 60983, 187: 21728, 188: 10418, 189: 21173, 190: 49278, 191: 41755, 192: 43992, 193: 13802, 194: 25009, 195: 36429, 196: 49744, 197: 61727, 198: 52247, 199: 4435, 200: 27083, 201: 48025, 202: 5234, 203: 21924, 204: 27614, 205: 437, 206: 812, 207: 11567, 208: 51061, 209: 61811, 210: 38464, 211: 43291, 212: 2465, 213: 1893, 214: 64637, 215: 39214, 216: 12398, 217: 3574, 218: 16350, 219: 53972, 220: 25150, 221: 9101, 222: 19505, 223: 6381, 224: 31855, 225: 59595, 226: 17379, 227: 2440, 228: 4576, 229: 62297, 230: 19521, 231: 11753, 232: 60217, 233: 13451, 234: 13409, 235: 32216, 236: 33526, 237: 30535, 238: 28554, 239: 15075, 240: 52718, 241: 39257, 242: 12539, 243: 48932, 244: 12415, 245: 24694, 246: 53165, 247: 12149, 248: 20019, 249: 30172, 250: 17467, 251: 47215, 252: 24234, 253: 4309, 254: 58560, 255: 22773, 256: 65559, 257: 38601, 258: 49629, 259: 35151, 260: 12103, 261: 8399, 262: 8306, 263: 7000, 264: 27929, 265: 26296, 266: 9868, 267: 24959, 268: 24181, 269: 25906, 270: 8497, 271: 20798, 272: 32205, 273: 35932, 274: 24022, 275: 16708, 276: 28104, 277: 30366, 278: 37263, 279: 34472, 280: 22633, 281: 21206, 282: 29874, 283: 6948, 284: 3686, 285: 53586, 286: 18951, 287: 3281, 288: 18821, 289: 68107, 290: 6939, 291: 37211, 292: 35840, 293: 50253, 294: 1596, 295: 25722, 296: 62346, 297: 25879, 298: 24447, 299: 36665, 300: 50512, 301: 50411, 302: 50620, 303: 16156, 304: 3748, 305: 62389, 306: 5483, 307: 14463, 308: 15410, 309: 8120, 310: 52056, 311: 13052, 312: 45375, 313: 9136, 314: 16517, 315: 39281, 316: 5173, 317: 3941, 318: 46217, 319: 21774, 320: 33456, 321: 5951, 322: 48202, 323: 17056, 324: 25365, 325: 19302, 326: 30309, 327: 42712, 328: 15499, 329: 12962, 330: 6182, 331: 47015, 332: 24091, 333: 25676, 334: 13507, 335: 6683, 336: 18743, 337: 38485, 338: 33447, 339: 28431, 340: 2993, 341: 75014, 342: 15078, 343: 12503, 344: 25519, 345: 45848, 346: 17439, 347: 43871, 348: 53735, 349: 2177, 350: 15245, 351: 50220, 352: 4217, 353: 42859, 354: 13272, 355: 823, 356: 21138, 357: 44006, 358: 25208, 359: 44795, 360: 43129, 361: 19402, 362: 21261, 363: 40296, 364: 14946, 365: 17605, 366: 11334, 367: 33697, 368: 6244, 369: 48508, 370: 22935, 371: 16443, 372: 16982, 373: 5161, 374: 36468, 376: 6048, 377: 43331, 378: 46837, 379: 40322, 380: 26324, 381: 45011, 382: 38253, 383: 1395, 384: 41937, 385: 1327, 386: 36790, 387: 65648, 388: 39440, 389: 9563, 390: 26992, 391: 42418, 392: 14996, 393: 16791, 394: 2260, 395: 18616, 396: 3799, 397: 26724, 398: 11682, 399: 20340, 400: 2353, 401: 7765, 402: 4414, 403: 678, 404: 35736, 405: 28834, 406: 18069, 407: 16589, 408: 63270, 409: 6531, 410: 32158, 411: 3048, 412: 21516, 413: 38218, 414: 31743, 415: 33115, 416: 11319, 417: 37948, 418: 20127, 419: 23116, 420: 2340, 421: 29992, 422: 668, 423: 17881, 424: 12954, 425: 14873, 426: 23181, 427: 23975, 428: 19079, 429: 54983, 430: 18720, 431: 22878, 432: 54220, 433: 17306, 434: 32815, 435: 36813, 436: 31783, 438: 23517, 439: 32967, 440: 39225, 441: 12934, 442: 56764, 443: 12014, 444: 29963, 445: 56206, 446: 8803, 447: 43647, 448: 55244, 449: 27016, 450: 16154, 451: 10257, 452: 49640, 453: 32471, 454: 38202, 455: 36441, 456: 2030, 457: 41969, 458: 31306, 459: 51388, 460: 27221, 461: 24010, 462: 75809, 463: 33651, 464: 524, 465: 13729, 466: 28271, 467: 9147, 468: 6541, 469: 22884, 470: 21873, 471: 37741, 472: 36171, 473: 12849, 474: 4764, 475: 54096, 476: 27482, 477: 1111, 478: 27442, 479: 46884, 480: 2392, 481: 9594, 482: 9860, 483: 9266, 484: 27167, 485: 4225, 486: 48530, 487: 3798, 488: 11967, 489: 31972, 490: 51555, 491: 4520, 492: 28940, 493: 28305, 494: 13631, 495: 5786, 496: 19524, 497: 46723, 498: 31806, 499: 11053, 500: 23903, 501: 13170, 502: 74658, 503: 26563, 504: 13464, 505: 69789, 506: 60199, 507: 62781, 508: 26846, 509: 58714, 510: 25972, 511: 31252, 512: 52081, 513: 22833, 514: 11362, 515: 11072, 516: 25044, 517: 29501, 518: 40336, 519: 2537, 520: 1801, 521: 18496, 522: 19967, 523: 16053, 525: 54030, 526: 26671, 527: 69130, 528: 10086, 529: 26674, 530: 72962, 531: 9339, 532: 20566, 533: 36427, 534: 42140, 535: 54016, 536: 44385, 537: 929, 538: 21062, 539: 10075, 540: 1610, 541: 25346, 542: 45282, 543: 16142, 544: 28527, 545: 3286, 546: 17376, 547: 9713, 548: 15293, 549: 38432, 550: 26500, 551: 50711, 552: 4336, 553: 20580, 554: 10820, 555: 2084, 556: 22193, 557: 36964, 558: 4199, 559: 24479, 560: 37698, 561: 6000, 562: 17778, 563: 3124, 565: 41349, 566: 15868, 567: 3448, 568: 65094, 569: 13563, 570: 6823, 571: 62139, 572: 29505, 573: 46461, 574: 15841, 575: 23852, 576: 11081, 577: 38814, 578: 17985, 579: 8917, 580: 27361, 581: 25118, 582: 46409, 583: 34827, 584: 4375, 585: 30748, 586: 58185, 587: 44720, 588: 25326, 589: 5162, 590: 32742, 591: 24453, 592: 5647, 593: 17549, 594: 23297, 595: 27661, 596: 39737, 597: 5005, 598: 19666, 599: 36115, 600: 22648, 601: 55512, 602: 48954, 603: 33787, 604: 33486, 605: 26156, 606: 53201, 607: 4157, 608: 12095, 609: 6912, 610: 5348, 611: 6350, 612: 36706, 613: 12595, 614: 41363, 615: 19524, 616: 21382, 617: 6223, 618: 3651, 619: 25957, 620: 10855, 621: 28140, 622: 671, 623: 2984, 624: 32392, 625: 9715, 626: 22485, 627: 23636, 628: 4699, 629: 38738, 630: 16147, 631: 27684, 632: 5637, 633: 34872, 634: 33423, 635: 14815, 636: 1900, 637: 21702, 638: 28312, 639: 18108, 640: 18150, 641: 38680, 642: 5410, 643: 26143, 644: 43667, 645: 17518, 646: 54010, 647: 9494, 648: 47749, 649: 18304, 650: 25890, 651: 34828, 652: 24398, 653: 71821, 654: 12258, 655: 54989, 656: 42692, 657: 19831, 658: 14476, 659: 23591, 660: 809, 661: 9291, 662: 55496, 663: 20337, 664: 25311, 665: 2272, 666: 10944, 667: 12651, 669: 14555, 670: 41008, 672: 48688, 673: 9037, 674: 8774, 675: 2077, 676: 7032, 677: 56115, 679: 41653, 680: 3851, 681: 21478, 682: 16360, 683: 51808, 684: 57373, 685: 1634, 686: 31929, 687: 9572, 688: 55858, 689: 26233, 690: 57294, 691: 1013, 692: 3958, 693: 43091, 694: 28721, 695: 48901, 696: 61204, 697: 42098, 698: 64647, 700: 33208, 701: 22099, 702: 33481, 703: 24247, 704: 12262, 705: 3215, 706: 59870, 707: 45465, 708: 43808, 709: 39752, 710: 30953, 711: 67326, 712: 63412, 713: 56398, 714: 62434, 715: 27980, 716: 3963, 717: 63292, 718: 11463, 719: 4411, 720: 31379, 721: 40496, 722: 20961, 723: 12015, 724: 36762, 725: 7512, 726: 17586, 727: 19284, 728: 32850, 729: 22090, 730: 28491, 731: 12977, 732: 45793, 733: 16521, 734: 12806, 735: 33986, 736: 71453, 737: 23793, 738: 16055, 739: 2973, 740: 8500, 741: 5413, 742: 78979, 743: 4767, 744: 45387, 745: 28073, 746: 54576, 747: 21821, 748: 40582, 749: 1666, 750: 34418, 751: 46511, 752: 8102, 753: 47703, 754: 41322, 755: 4053, 756: 17756, 757: 49751, 758: 34909, 759: 35830, 760: 24839, 761: 25145, 762: 36628, 763: 9064, 764: 3807, 765: 18389, 766: 55768, 767: 15998, 768: 57478, 769: 6248, 770: 31161, 771: 52399, 772: 37486, 773: 43160, 774: 74994, 775: 1116, 776: 12758, 777: 56805, 778: 23146, 779: 44696, 780: 1737, 781: 49205, 782: 43014, 783: 20142, 784: 39719, 785: 10510, 786: 8202, 787: 40131, 788: 21159, 789: 64431, 790: 23961, 791: 11082, 792: 16395, 793: 6401, 794: 20888, 795: 41135, 796: 13265, 797: 15405, 798: 27781, 799: 2113, 800: 16433, 801: 44924, 802: 9724, 803: 37371, 804: 19769, 805: 49267, 806: 42750, 807: 47473, 808: 11512, 810: 5834, 811: 1935, 813: 35278, 814: 17241, 815: 52156, 816: 30903, 817: 32366, 818: 24945, 819: 23832, 820: 27942, 821: 21379, 822: 63601, 824: 32839, 825: 7299, 826: 13431, 827: 39480, 828: 16684, 829: 8936, 830: 22334, 831: 61901, 832: 35882, 833: 1566, 834: 63537, 835: 38287, 836: 61251, 837: 20054, 838: 11258, 839: 58628, 840: 71983, 841: 5953, 842: 35879, 843: 41484, 844: 41473, 845: 25490, 846: 23771, 847: 38663, 848: 20777, 849: 5735, 850: 24633, 851: 17217, 852: 20229, 853: 41718, 854: 40830, 855: 1609, 856: 11978, 857: 45331, 858: 41004, 859: 38958, 860: 30593, 861: 22246, 862: 14540, 863: 31397, 864: 21031, 865: 12333, 866: 5366, 867: 21735, 868: 43397, 869: 32172, 870: 41610, 871: 5392, 872: 34805, 873: 37516, 874: 32941, 875: 54228, 876: 25304, 877: 8689, 878: 37664, 879: 6748, 880: 926, 881: 16376, 882: 7798, 883: 24357, 884: 5458, 885: 26010, 886: 33958, 887: 16726, 888: 15838, 889: 3588, 890: 54498, 891: 45641, 892: 15889, 893: 59138, 894: 13221, 896: 33017, 897: 45044, 898: 3333, 899: 20201, 900: 8587, 901: 20704, 902: 27128, 903: 23513, 904: 30738, 905: 13005, 906: 41148, 907: 19177, 908: 7220, 909: 71454, 910: 44388, 911: 44431, 912: 11619, 913: 1574, 914: 12961, 915: 31909, 916: 15992, 917: 18156, 918: 18989, 919: 53350, 920: 35733, 921: 31179, 922: 21027, 923: 34835, 924: 60150, 925: 18445, 927: 16940, 928: 32192, 930: 33578, 931: 9548, 933: 28044, 934: 11300, 935: 4072, 936: 25404, 937: 10182, 938: 28235, 939: 10209, 940: 13471, 941: 30671, 942: 2779, 943: 21953, 944: 32998, 945: 53500, 946: 39625, 947: 11444, 948: 11883, 949: 39141, 950: 41904, 951: 11929, 952: 22705, 953: 75243, 954: 19376, 955: 23937, 956: 42485, 957: 47711, 958: 28445, 959: 16846, 960: 51608, 961: 9139, 962: 8345, 963: 19031, 964: 30694, 965: 13954, 966: 2089, 967: 13178, 968: 30219, 969: 64576, 970: 37462, 971: 13548, 972: 38490, 973: 7126, 974: 5653, 975: 28891, 976: 33332, 977: 42768, 978: 9381, 979: 36998, 980: 42602, 981: 25836, 982: 31377, 983: 60394, 984: 13154, 985: 25139, 986: 1829, 987: 23201, 988: 70407, 989: 55102, 990: 27846, 991: 27001, 992: 39972, 993: 56380, 994: 24429, 995: 65659, 996: 35131, 997: 37013, 998: 7966, 999: 18580}\n"
     ]
    }
   ],
   "source": [
    "best_order = {}\n",
    "processed_indices = set()\n",
    "\n",
    "\n",
    "for i in range(len(sorted_data[:1000])):\n",
    "    if i in processed_indices:\n",
    "        continue\n",
    "\n",
    "    common_tags = {}\n",
    "    for j in range(i+1, len(sorted_data)):\n",
    "        counter = 0\n",
    "        for element in sorted_data[i][3]:\n",
    "            if element in sorted_data[j][3]:\n",
    "                counter += 1\n",
    "                common_tags[j] = counter\n",
    "    \n",
    "    max_common_tags = max(common_tags.values(), default=0)\n",
    "    if max_common_tags > 0:\n",
    "        best_order[i] = max(common_tags, key=common_tags.get)\n",
    "        processed_indices.add(best_order[i])\n",
    "\n",
    "print(best_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data to get the max tags in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the max\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Define a function to process a chunk of data within a specified range\n",
    "def process_chunk_range(data_range):\n",
    "    chunk_start, chunk_end = data_range\n",
    "    chunk = data[chunk_start:chunk_end]\n",
    "    chunk_best_order = {}\n",
    "    for i, index in enumerate(range(chunk_start, chunk_end)):\n",
    "        current_tags = set(chunk[i][2])\n",
    "        common_tags_count = {}\n",
    "        \n",
    "        # Loop over subsequent rows to count common tags\n",
    "        for j in range(i + 1, len(chunk)):\n",
    "            next_tags = set(chunk[j][2])\n",
    "            common_tags_count[j] = len(current_tags.intersection(next_tags))\n",
    "        \n",
    "        # Filter out zero values\n",
    "        non_zero_values = [value for value in common_tags_count.values() if value != 0]\n",
    "        if not non_zero_values:\n",
    "            continue\n",
    "        \n",
    "        # Find the minimum common tags count\n",
    "        max_common_tags = max(non_zero_values)\n",
    "        \n",
    "        # Find the index of the next row with the minimum common tags\n",
    "        next_row_index = max(common_tags_count, key=lambda k: (common_tags_count[k] == max_common_tags, k))\n",
    "        \n",
    "        chunk_best_order[index] = next_row_index\n",
    "    return chunk_best_order\n",
    "\n",
    "# Define the chunk size and the number of worker threads\n",
    "chunk_size = 8000\n",
    "num_threads = cpu_count()\n",
    "\n",
    "# Divide the data into ranges of indices\n",
    "data_ranges = [(i, min(i + chunk_size, len(data))) for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "# Process each range in parallel using ThreadPoolExecutor\n",
    "best_order = {}\n",
    "import time\n",
    "st = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(process_chunk_range, data_range) for data_range in data_ranges]\n",
    "    for future in as_completed(futures):\n",
    "        chunk_best_order = future.result()\n",
    "        best_order.update(chunk_best_order)\n",
    "\n",
    "et = time.time()\n",
    "elapsed_time = round((et - st)/60,2)\n",
    "print('Execution time:', elapsed_time, 'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tags to have pairs with least in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the min\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Define a function to process a chunk of data within a specified range\n",
    "def process_chunk_range(data_range):\n",
    "    chunk_start, chunk_end = data_range\n",
    "    chunk = data[chunk_start:chunk_end]\n",
    "    chunk_best_order = {}\n",
    "    for i, index in enumerate(range(chunk_start, chunk_end)):\n",
    "        current_tags = set(chunk[i][2])\n",
    "        common_tags_count = {}\n",
    "        \n",
    "        # Loop over subsequent rows to count common tags\n",
    "        for j in range(i + 1, len(chunk)):\n",
    "            next_tags = set(chunk[j][2])\n",
    "            common_tags_count[j] = len(current_tags.intersection(next_tags))\n",
    "        \n",
    "        # Filter out zero values\n",
    "        non_zero_values = [value for value in common_tags_count.values() if value != 0]\n",
    "        if not non_zero_values:\n",
    "            continue\n",
    "        \n",
    "        # Find the minimum common tags count\n",
    "        min_common_tags = min(non_zero_values)\n",
    "        \n",
    "        # Find the index of the next row with the minimum common tags\n",
    "        next_row_index = min(common_tags_count, key=lambda k: (common_tags_count[k] == min_common_tags, k))\n",
    "        \n",
    "        chunk_best_order[index] = next_row_index\n",
    "    return chunk_best_order\n",
    "\n",
    "# Define the chunk size and the number of worker threads\n",
    "chunk_size = 8000\n",
    "num_threads = cpu_count()\n",
    "\n",
    "# Divide the data into ranges of indices\n",
    "data_ranges = [(i, min(i + chunk_size, len(data))) for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "# Process each range in parallel using ThreadPoolExecutor\n",
    "best_order = {}\n",
    "import time\n",
    "st = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(process_chunk_range, data_range) for data_range in data_ranges]\n",
    "    for future in as_completed(futures):\n",
    "        chunk_best_order = future.result()\n",
    "        best_order.update(chunk_best_order)\n",
    "\n",
    "et = time.time()\n",
    "elapsed_time = round((et - st)/60,2)\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tags to have min in common but iterates until either data is empty or no pairs are being created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can process up to 60k rows out of 80k in 10mins, after 60k less and less pairs so taking a lot of time.\n",
    "\n",
    "For now we can create as much pairs as we can, shuffle the rest and add in a random order to see score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "df = sorted_data\n",
    "\n",
    "# Define the function to process each chunk range\n",
    "def process_chunk_range(data_range, data):\n",
    "    chunk_start, chunk_end = data_range\n",
    "    chunk = data[chunk_start:chunk_end]\n",
    "    chunk_best_order = {}\n",
    "    \n",
    "    # Loop through each row in the chunk\n",
    "    for i, index in enumerate(range(chunk_start, chunk_end)):\n",
    "        current_tags = set(chunk[i][3])\n",
    "        common_tags_count = {}\n",
    "        \n",
    "        # Loop over subsequent rows to count common tags\n",
    "        for j in range(i + 1, len(chunk)):\n",
    "            next_tags = set(chunk[j][3])\n",
    "            common_tags_count[j] = len(current_tags.intersection(next_tags))\n",
    "        \n",
    "        # Filter out zero values\n",
    "        non_zero_values = [value for value in common_tags_count.values() if value != 0]\n",
    "        if not non_zero_values:\n",
    "            continue\n",
    "        \n",
    "        # Find the minimum common tags count\n",
    "        min_common_tags = min(non_zero_values)\n",
    "        \n",
    "        # Find the index of the next row with the minimum common tags\n",
    "        next_row_index = min(common_tags_count, key=lambda k: (common_tags_count[k] == min_common_tags, k))\n",
    "        \n",
    "        # Update the chunk_best_order dictionary\n",
    "        chunk_best_order[chunk[i][0]] = chunk[next_row_index][0]\n",
    "    \n",
    "    return chunk_best_order\n",
    "\n",
    "# Main loop to process until df is empty\n",
    "total_time = 0\n",
    "processed_indexes = set()\n",
    "all_best_order = {}\n",
    "while True:\n",
    "    # Define the chunk size and the number of worker threads\n",
    "    chunk_size = 8000\n",
    "    num_threads = cpu_count()\n",
    "\n",
    "    # Divide the data into ranges of indices\n",
    "    data_ranges = [(i, min(i + chunk_size, len(df))) for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    # Process each range in parallel using ThreadPoolExecutor\n",
    "    best_order = {}\n",
    "    st = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(process_chunk_range, data_range, df) for data_range in data_ranges]\n",
    "        for future in as_completed(futures):\n",
    "            chunk_best_order = future.result()\n",
    "            best_order.update(chunk_best_order)\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = round((et - st)/60, 2)\n",
    "    print('Execution time:', elapsed_time, 'minutes')\n",
    "    total_time += elapsed_time\n",
    "\n",
    "    # Add processed indexes to the set\n",
    "    for key, value in best_order.items():\n",
    "        processed_indexes.add(key)\n",
    "        processed_indexes.add(value)\n",
    "\n",
    "    # Remove processed rows from df in order\n",
    "    df = [row for row in df if row[0] not in processed_indexes]\n",
    "\n",
    "    # Save the current best_order to all_best_order\n",
    "    all_best_order.update(best_order)\n",
    "\n",
    "    # Check if any pairs were found\n",
    "    if not best_order:\n",
    "        break  # No pairs found, exit the loop\n",
    "\n",
    "print(\"All data processed.\")\n",
    "print(f\"Total Time needed: {total_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = len(processed_indexes) + len(df)\n",
    "import random\n",
    "random.shuffle(df)\n",
    "\n",
    "shuffled_frames = {}\n",
    "for i in range(len(df)):\n",
    "    shuffled_frames[i] = df[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file for writing at the specified path\n",
    "with open(\"../data/binary_landscapes_output_2.txt\", \"w\") as f:\n",
    "\n",
    "    keys_list = list(all_best_order.keys())\n",
    "    f.write(str(total_length) + \"\\n\")\n",
    "\n",
    "    current_key = keys_list[0]\n",
    "    i = 0\n",
    "\n",
    "    while i < total_length and keys_list:\n",
    "        # Write the current key's value to the file\n",
    "        print(f\"Writing key {current_key} to file\")\n",
    "        f.write(str(current_key) + \"\\n\")\n",
    "        next_key = all_best_order[current_key]\n",
    "        keys_list.remove(current_key)\n",
    "        if next_key in keys_list:\n",
    "            current_key = next_key\n",
    "        else:\n",
    "            current_key = keys_list[0] if keys_list else None\n",
    "        i += 1\n",
    "\n",
    "with open(\"../data/binary_landscapes_output_2.txt\", \"a\") as f:\n",
    "\n",
    "    keys_list = list(shuffled_frames.keys())\n",
    "\n",
    "    current_key = 0\n",
    "    i = 0\n",
    "\n",
    "    while keys_list:\n",
    "        # Write the current key's value to the file\n",
    "        print(f\"Writing key {current_key} to file\")\n",
    "        f.write(str(current_key) + \"\\n\")\n",
    "        next_key = shuffled_frames[current_key]\n",
    "        keys_list.remove(current_key)\n",
    "        if next_key in keys_list:\n",
    "            current_key = next_key\n",
    "        else:\n",
    "            current_key = keys_list[0] if keys_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove duplicates from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file for reading\n",
    "file_path = \"/Users/julien/Documents/EPITA/S2/kaggle_week/data/binary_landscapes_output_2.txt\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    # Read all lines from the file\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Remove duplicates by creating a set\n",
    "unique_lines = set(lines)\n",
    "len(unique_lines)\n",
    "\n",
    "# Open the file again for writing (this will overwrite the existing file)\n",
    "with open(file_path, \"w\") as f:\n",
    "    # Write the unique lines back to the file\n",
    "    f.write(str(len(unique_lines)) + \"\\n\")\n",
    "    f.writelines(unique_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /!\\ STOP HERE /!\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this length of sorted_data before deletion: 75000\n",
      "this length of sorted_data after deletion: 74000\n",
      "this length of sorted_data before deletion: 74000\n",
      "this length of sorted_data after deletion: 73000\n",
      "this length of sorted_data before deletion: 73000\n",
      "this length of sorted_data after deletion: 72000\n",
      "this length of sorted_data before deletion: 72000\n",
      "this length of sorted_data after deletion: 71000\n",
      "this length of sorted_data before deletion: 71000\n",
      "this length of sorted_data after deletion: 70000\n",
      "this length of sorted_data before deletion: 70000\n",
      "this length of sorted_data after deletion: 69000\n",
      "this length of sorted_data before deletion: 69000\n",
      "this length of sorted_data after deletion: 68000\n",
      "this length of sorted_data before deletion: 68000\n",
      "this length of sorted_data after deletion: 67000\n",
      "this length of sorted_data before deletion: 67000\n",
      "this length of sorted_data after deletion: 66000\n",
      "this length of sorted_data before deletion: 66000\n",
      "this length of sorted_data after deletion: 65000\n",
      "this length of sorted_data before deletion: 65000\n",
      "this length of sorted_data after deletion: 64000\n",
      "this length of sorted_data before deletion: 64000\n",
      "this length of sorted_data after deletion: 63000\n",
      "this length of sorted_data before deletion: 63000\n",
      "this length of sorted_data after deletion: 62000\n",
      "this length of sorted_data before deletion: 62000\n",
      "this length of sorted_data after deletion: 61000\n",
      "this length of sorted_data before deletion: 61000\n",
      "this length of sorted_data after deletion: 60000\n",
      "this length of sorted_data before deletion: 60000\n",
      "this length of sorted_data after deletion: 59000\n",
      "this length of sorted_data before deletion: 59000\n",
      "this length of sorted_data after deletion: 58000\n",
      "this length of sorted_data before deletion: 58000\n",
      "this length of sorted_data after deletion: 57000\n",
      "this length of sorted_data before deletion: 57000\n",
      "this length of sorted_data after deletion: 56000\n",
      "this length of sorted_data before deletion: 56000\n",
      "this length of sorted_data after deletion: 55000\n",
      "this length of sorted_data before deletion: 55000\n",
      "this length of sorted_data after deletion: 54000\n",
      "this length of sorted_data before deletion: 54000\n",
      "this length of sorted_data after deletion: 53000\n",
      "this length of sorted_data before deletion: 53000\n",
      "this length of sorted_data after deletion: 52000\n",
      "this length of sorted_data before deletion: 52000\n",
      "this length of sorted_data after deletion: 51000\n",
      "this length of sorted_data before deletion: 51000\n",
      "this length of sorted_data after deletion: 50000\n",
      "this length of sorted_data before deletion: 50000\n",
      "this length of sorted_data after deletion: 49000\n",
      "this length of sorted_data before deletion: 49000\n",
      "this length of sorted_data after deletion: 48000\n",
      "this length of sorted_data before deletion: 48000\n",
      "this length of sorted_data after deletion: 47000\n",
      "this length of sorted_data before deletion: 47000\n",
      "this length of sorted_data after deletion: 46000\n",
      "this length of sorted_data before deletion: 46000\n",
      "this length of sorted_data after deletion: 45000\n",
      "this length of sorted_data before deletion: 45000\n",
      "this length of sorted_data after deletion: 44000\n",
      "this length of sorted_data before deletion: 44000\n",
      "this length of sorted_data after deletion: 43000\n",
      "this length of sorted_data before deletion: 43000\n",
      "this length of sorted_data after deletion: 42000\n",
      "this length of sorted_data before deletion: 42000\n",
      "this length of sorted_data after deletion: 41000\n",
      "this length of sorted_data before deletion: 41000\n",
      "this length of sorted_data after deletion: 40000\n",
      "this length of sorted_data before deletion: 40000\n",
      "this length of sorted_data after deletion: 39000\n",
      "this length of sorted_data before deletion: 39000\n",
      "this length of sorted_data after deletion: 38000\n",
      "this length of sorted_data before deletion: 38000\n",
      "this length of sorted_data after deletion: 37000\n",
      "this length of sorted_data before deletion: 37000\n",
      "this length of sorted_data after deletion: 36000\n",
      "this length of sorted_data before deletion: 36000\n",
      "this length of sorted_data after deletion: 35000\n",
      "this length of sorted_data before deletion: 35000\n",
      "this length of sorted_data after deletion: 34000\n",
      "this length of sorted_data before deletion: 34000\n",
      "this length of sorted_data after deletion: 33000\n",
      "this length of sorted_data before deletion: 33000\n",
      "this length of sorted_data after deletion: 32000\n",
      "this length of sorted_data before deletion: 32000\n",
      "this length of sorted_data after deletion: 31000\n",
      "this length of sorted_data before deletion: 31000\n",
      "this length of sorted_data after deletion: 30000\n",
      "this length of sorted_data before deletion: 30000\n",
      "this length of sorted_data after deletion: 29000\n",
      "this length of sorted_data before deletion: 29000\n",
      "this length of sorted_data after deletion: 28000\n",
      "this length of sorted_data before deletion: 28000\n",
      "this length of sorted_data after deletion: 27000\n",
      "this length of sorted_data before deletion: 27000\n",
      "this length of sorted_data after deletion: 26000\n",
      "this length of sorted_data before deletion: 26000\n",
      "this length of sorted_data after deletion: 25000\n",
      "this length of sorted_data before deletion: 25000\n",
      "this length of sorted_data after deletion: 24000\n",
      "this length of sorted_data before deletion: 24000\n",
      "this length of sorted_data after deletion: 23000\n",
      "this length of sorted_data before deletion: 23000\n",
      "this length of sorted_data after deletion: 22000\n",
      "this length of sorted_data before deletion: 22000\n",
      "this length of sorted_data after deletion: 21000\n",
      "this length of sorted_data before deletion: 21000\n",
      "this length of sorted_data after deletion: 20000\n",
      "this length of sorted_data before deletion: 20000\n",
      "this length of sorted_data after deletion: 19000\n",
      "this length of sorted_data before deletion: 19000\n",
      "this length of sorted_data after deletion: 18000\n",
      "this length of sorted_data before deletion: 18000\n",
      "this length of sorted_data after deletion: 17000\n",
      "this length of sorted_data before deletion: 17000\n",
      "this length of sorted_data after deletion: 16000\n",
      "this length of sorted_data before deletion: 16000\n",
      "this length of sorted_data after deletion: 15000\n",
      "this length of sorted_data before deletion: 15000\n",
      "this length of sorted_data after deletion: 14000\n",
      "this length of sorted_data before deletion: 14000\n",
      "this length of sorted_data after deletion: 13000\n",
      "this length of sorted_data before deletion: 13000\n",
      "this length of sorted_data after deletion: 12000\n",
      "this length of sorted_data before deletion: 12000\n",
      "this length of sorted_data after deletion: 11000\n",
      "this length of sorted_data before deletion: 11000\n",
      "this length of sorted_data after deletion: 10000\n",
      "this length of sorted_data before deletion: 10000\n",
      "this length of sorted_data after deletion: 9000\n",
      "this length of sorted_data before deletion: 9000\n",
      "this length of sorted_data after deletion: 8000\n",
      "this length of sorted_data before deletion: 8000\n",
      "this length of sorted_data after deletion: 7000\n",
      "this length of sorted_data before deletion: 7000\n",
      "this length of sorted_data after deletion: 6000\n",
      "this length of sorted_data before deletion: 6000\n",
      "this length of sorted_data after deletion: 5000\n",
      "this length of sorted_data before deletion: 5000\n",
      "this length of sorted_data after deletion: 4000\n",
      "this length of sorted_data before deletion: 4000\n",
      "this length of sorted_data after deletion: 3000\n",
      "this length of sorted_data before deletion: 3000\n",
      "this length of sorted_data after deletion: 2000\n",
      "this length of sorted_data before deletion: 2000\n",
      "this length of sorted_data after deletion: 1000\n",
      "this length of sorted_data before deletion: 1000\n",
      "this length of sorted_data after deletion: 0\n"
     ]
    }
   ],
   "source": [
    "# base loop, don't execute it runs for ever\n",
    "while len(sorted_data)>0:\n",
    "    for i in range(len(sorted_data[:1000])):\n",
    "\n",
    "        common_tags = {}\n",
    "        for j in range(i+1, len(sorted_data)):\n",
    "            counter = 0\n",
    "            for element in sorted_data[i][3]:\n",
    "                if element in sorted_data[j][3]:\n",
    "                    counter += 1\n",
    "                    common_tags[j] = counter\n",
    "        \n",
    "        max_common_tags = min(common_tags.values(), default=0)\n",
    "        if max_common_tags > 0:\n",
    "            best_order[i] = max(common_tags, key=common_tags.get)\n",
    "            \n",
    "    print(f\"this length of sorted_data before deletion: {len(sorted_data)}\")\n",
    "    del sorted_data[:1000]\n",
    "    print(f\"this length of sorted_data after deletion: {len(sorted_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
